# Frontier Tester

Wrapper to test failure recovery in [frontier](https://github.com/dgc-rhul/frontier).

## Install
* Intall [NodeJS](https://nodejs.org/en/download/)
* Intall frontier from [https://github.com/nicolatoscan/frontier](https://github.com/nicolatoscan/frontier), a fork variant of frontier made to be able to communicate to this wrapper.
* Install all necessary dependencies with `npm install`
* Update the `FRONTIER_PATH` variable in the `.env` file with the path to the frontier folder

## Run
There are two ways to run the wrapper: as CLI or as a library.


### CLI
To run as CLI:
```bash
npx ts-node src/Tester.ts
```

The following options are available

- `--folder <folder>` or `-f <folder>`
  - Folder where to store the experiment results. If not specified, the results will be stored in a folder generated with all experiment variables.
- `--tuples <n>` or `-t <n>`
  - Number of tuples generated by the source node. Default: `20000`
- `--replication <n>` or `-r <n>`
  - Number of workers on each level. Default: `2`
- `--length <n>` or `-l <n>`
  - Number of levels in the chain. Default: `1`
- `--warmup <n>` or `-w <n>`
  - Number of warmup tuples. Default: `0`
- `--kill` or `-k`
  - If specified, the experiment will kill workers at regular intervals. Default: `false`
- `--maxSrcQueue` or `-s`
  - Maximum size of the source node queue. Default: `100`
- `--maxQueue` or `-q`
  - Maximum size of the worker node queue. Default: `100`
- `--rateLimitSrc` or `-x`
  - If specified, the source node will rate limit the tuples it sends to the workers at 1000 tuples/s. Default: `false`


### Library
To use as a library:
```typescript
import FrontierTester, { FrontierTesterProps } from './Tester';

async function experiment() {
  const props: FrontierTesterProps = {
    n: 1000,
    numTuples: 100000,
    replicationFactor: 2,
    chainLength: 1,
    kill: false,
    rateLimitSrc: true,
    maxSrcTotalQueueSizeTuples: 100,
    maxTotalQueueSizeTuples: 100 
  };
  const tester = new FrontierTester(prop);
  await tester.run();
}


experiment();
```
An example of how to use the library is available in `src/index.ts`, where multiple experiments are run.

## Results
After an experiment is run, 4 files are generated in the specified folder:
- `sink.csv`
  - Contains the tuples received by the sink node as a CSV file with 12 columns:
    - `PY`: this column is always PY and is used to identify the data in the sink stdout
    - `Workername`: this column is always SINK and is used to identify the data in the sink stdout
    - `count`: tuple count as received
    - `id`: tuple id
    - `id`: tuple id
    - `txts`: tuple generation timestamp
    - `rxts`: tuple received timestamp
    - `latency`: milliseconds between tuple generation and reception
    - `bytes`: tuple size in bytes
    - `latencies`: `|`-separated list of latency values between each worker node
    - `processorIds`: `|`-separated list of workers ids values that the tuple went through
    - `debug`: this column is only used for debug data
- `monitoring.csv`
  - Contains the system monitoring data of CPU and Memory usage as a CSV file with 1 + #nodes columns:
    - `timestamp`: timestamp of the monitoring data
    - `workerInfo`: ','-separated list of `workerPid`, `CPU usage`, `Memory usage` for each worker node
- `event.csv`
  - Containes important events timestamps in the experiment as a CSV file with 2 columns:
    - `timestamp`: timestamp of the event
    - `name`: name of the event among: STARTED, KILLED, DONE
- `pidMap.csv`
  - Contains the mapping between the worker node PID and the worker nameof the worker as a CSV file with 2 columns:
    - `pid`: PID of the worker node
    - `name`: Worker's name among: SOURCE, SINK, PROCESSOR, PROCESSOR

## Analysis
To analyze the results, use the `scripts/plot.py` file.
The file is divided into 4 parts:
* import libraries
* function used to load data from files
* function used to plot data
* loading of all data alreaady generated

The following plotting functions are available:
* `plotLatency`: plot the latency of the tuples
  - `*data`: list of data of different experiments to plot
  - `ylim`: ylim for pyplot chart
  - `sameColor`: if false, each tuple point will have a color identifing its path through the chain
  - `imgFile`: if defined, the plot will be saved in the specified file
* `plotCpuMem`: plot the CPU and memeory usage
  - `*data`: list of data of different experiments to plot
  - `onlyLines`: if true the plot is a line plot, otherwise it is a stack plot
  - `imgFile`: if defined, the plot will be saved in the specified file
* `plotThroughput`: plot the throughput
  - `*data`: list of data of different experiments to plot
  - `ylim`: ylim for pyplot chart
  - `imgFile`: if defined, the plot will be saved in the specified file

Some examples of how to use the plotting functions are available in the last part of the file.

All plots for the experiments in the paper are available in the `results/plots/std` folder.